# -*- coding: utf-8 -*-
"""TP3 CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11odbHU7fMbIQr0hmrnvpsN_DmO_ujyCI

#Consigna
Identificar un problema de clasificación multiclase de imagenes que puedan resolver utilizando una red neuronal convolucional implementada en Keras utilizando TensorFlow como vimos en clase. Desarrollar un modelo para resolver en un Python Notebook, donde diseñen una red e implementen como adaptar los datos de entrada a una matriz, para pasarla por un modelo creado con Keras. Se pueden apoyar del ejemplo resuelto sobre el dataset de MNIST de clasificación de Digitos en este Notebook

Ademas en el mismo notebook deberán:

1. Explicar el problema a resolver y la solución propuesta
2. Identificar y explicar los conceptos teóricos vistos en clase sobre modelado y entrenamiento de redes neuronales. Tip: visualizar el modelo con algún paquete como keras-visualizer para explicar su comportamiento.
3. Visualizar los datos de entrada, ejemplos tomados del dataset, y algunos ejemplos de salida.

Se puede optar por resolver el problema con el dataset de Fashion MNIST. Este ya viene cargado en Google Colab. Pueden encontrar otros datasets en Kaggle o directamente de internet.

#Consigna 1
El dataset importado de keras contiene imagenes de 28x28, las cuales son imagenes diferentes prendas de ropa. Lo que vamos a hacer con este modelo es que aprenda a identificar prenda de ropa es en base a la foto.
"""

#Importamos las librerias necesarias
import tensorflow as tf
import seaborn as sns
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

#Importamos el dataset
fashion_mnist = tf.keras.datasets.fashion_mnist

#Importamos la data
(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()
print(y_train.shape, y_test.shape)

#Ploteamos la primer imagen del dataset
plt.imshow(X_train[0])

#Chequeamos el tamaño de la imagen
X_train[0].shape

#Chequeamos las dimensiones de los datos de entrenamiento
X_train.shape

#Modificamos los datos de entrenamiento para que se ajusten con el modelo
X_train = X_train.reshape(60000,28,28,1)
print(X_train.shape)

#Chequeamos las dimensiones de los datos de testeo
X_test.shape

#Modificamos los datos de testeo para que se ajusten con el modelo
X_test = X_test.reshape(10000,28,28,1)
print(X_test.shape)

#Usamos la función de TensorFlow para convertir las etiquetas de entrenamiento a one_hot encodign, y luego hacemos lo mismo con las de testeo
y_train_onehot = tf.one_hot(y_train, depth=10)
y_test_onehot = tf.one_hot(y_test, depth=10)
print("y_train[0]:", y_train[0])
print("y_train_onehot[0]:", y_train_onehot[0])

#Parámetros del modelo
num_filters = 8 #Cantidad de filtros en la capa de convolución
filter_size = 3 #Tamaño del filtro para la capa convolucional
pool_size = 2 #Tamaño de la ventana para la capa de max-pooling

#Definición del modelo secuencial
model = tf.keras.Sequential([
    tf.keras.Input(shape=(28, 28, 1)),  # La forma de la imagen es 28x28x1 (escala de grises)
    #Primera capa convolucional
    tf.keras.layers.Conv2D(
        filters=num_filters, #Aplicamos 8 filtros para extraer características
        kernel_size=filter_size,#Cada filtro es de tamaño 3x3
        activation='relu', #Utilizamos la activación ReLU
    ),
    #Capa de max-pooling
    tf.keras.layers.MaxPooling2D(
        pool_size=pool_size #Reducimos la dimensionalidad con una ventana de 2x2
    ),

    #Capa de Flatten
    tf.keras.layers.Flatten(), #Convertimos la salida 2D de la capa convolucional en un vector 1D

    # Primera capa densa (completamente conectada)
    tf.keras.layers.Dense(
        128, #Tiene 128 neuronas
        activation='relu' #Usamos la activación ReLU
    ),

    # Capa de salida
    tf.keras.layers.Dense(
        10, #Tiene 10 neuronas (una por clase en fashion_mnist)
        activation='softmax' #Softmax para obtener probabilidades para cada clase
    )
])

#Compilamos el modelo
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

#Entrenamos el modelo
history = model.fit(X_train, y_train_onehot, validation_data=(X_test, y_test_onehot), epochs=5)

history.history

#Grafico de la evolución del loss durante el entrenamiento en los epochs.
plt.plot(history.history['loss'])

fig, ax = plt.subplots(1,2, figsize=(10, 5))

ax[0].plot(history.history['loss'], color='b', label="Training Loss")
ax[0].plot(history.history['val_loss'], color='r', label="Validation Loss")
legend = ax[0].legend(loc='best', shadow=True)

ax[1].plot(history.history['accuracy'], color='b', label="Training Accuracy")
ax[1].plot(history.history['val_accuracy'], color='r',label="Validation Accuracy")
legend = ax[1].legend(loc='best', shadow=True)

#Predecimos todas las imagenes del dataset
pred_probs = model.predict(X_test)
pred_probs

#Para cada imagen, buscamos la etiqueta con mayor probabilidad.
pred_classes = np.argmax(pred_probs,axis=1)

#Mostramos las etiquetas
y_test